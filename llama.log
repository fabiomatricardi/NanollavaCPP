[1716096893] [C:\Users\FabioMatricardi\AppData\Local\Temp\pip-install-evovnb_5\llama-cpp-python_47ad063b60de4783b243782a39c9c6ac\vendor\llama.cpp\examples\llava\clip.cpp:  919][         clip_model_load] clip_model_load: loaded meta data with 19 key-value pairs and 457 tensors from nanolava/nanollava-mmproj-f16.gguf
[1716096893] [C:\Users\FabioMatricardi\AppData\Local\Temp\pip-install-evovnb_5\llama-cpp-python_47ad063b60de4783b243782a39c9c6ac\vendor\llama.cpp\examples\llava\clip.cpp:  929][         clip_model_load] clip_model_load: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
[1716096893] [C:\Users\FabioMatricardi\AppData\Local\Temp\pip-install-evovnb_5\llama-cpp-python_47ad063b60de4783b243782a39c9c6ac\vendor\llama.cpp\examples\llava\clip.cpp:  945][         clip_model_load] clip_model_load: - kv   0:                       general.architecture str              = clip
[1716096893] [C:\Users\FabioMatricardi\AppData\Local\Temp\pip-install-evovnb_5\llama-cpp-python_47ad063b60de4783b243782a39c9c6ac\vendor\llama.cpp\examples\llava\clip.cpp:  945][         clip_model_load] clip_model_load: - kv   1:                      clip.has_text_encoder bool             = false
[1716096893] [C:\Users\FabioMatricardi\AppData\Local\Temp\pip-install-evovnb_5\llama-cpp-python_47ad063b60de4783b243782a39c9c6ac\vendor\llama.cpp\examples\llava\clip.cpp:  945][         clip_model_load] clip_model_load: - kv   2:                    clip.has_vision_encoder bool             = true
[1716096893] [C:\Users\FabioMatricardi\AppData\Local\Temp\pip-install-evovnb_5\llama-cpp-python_47ad063b60de4783b243782a39c9c6ac\vendor\llama.cpp\examples\llava\clip.cpp:  945][         clip_model_load] clip_model_load: - kv   3:                   clip.has_llava_projector bool             = true
[1716096893] [C:\Users\FabioMatricardi\AppData\Local\Temp\pip-install-evovnb_5\llama-cpp-python_47ad063b60de4783b243782a39c9c6ac\vendor\llama.cpp\examples\llava\clip.cpp:  945][         clip_model_load] clip_model_load: - kv   4:                          general.file_type u32              = 1
[1716096893] [C:\Users\FabioMatricardi\AppData\Local\Temp\pip-install-evovnb_5\llama-cpp-python_47ad063b60de4783b243782a39c9c6ac\vendor\llama.cpp\examples\llava\clip.cpp:  945][         clip_model_load] clip_model_load: - kv   5:                               general.name str              = qnguyen3/nanoLLaVA
[1716096893] [C:\Users\FabioMatricardi\AppData\Local\Temp\pip-install-evovnb_5\llama-cpp-python_47ad063b60de4783b243782a39c9c6ac\vendor\llama.cpp\examples\llava\clip.cpp:  945][         clip_model_load] clip_model_load: - kv   6:                        general.description str              = image encoder for qnguyen3/nanoLLaVA
[1716096893] [C:\Users\FabioMatricardi\AppData\Local\Temp\pip-install-evovnb_5\llama-cpp-python_47ad063b60de4783b243782a39c9c6ac\vendor\llama.cpp\examples\llava\clip.cpp:  945][         clip_model_load] clip_model_load: - kv   7:                        clip.projector_type str              = mlp
[1716096893] [C:\Users\FabioMatricardi\AppData\Local\Temp\pip-install-evovnb_5\llama-cpp-python_47ad063b60de4783b243782a39c9c6ac\vendor\llama.cpp\examples\llava\clip.cpp:  945][         clip_model_load] clip_model_load: - kv   8:                     clip.vision.image_size u32              = 378
[1716096893] [C:\Users\FabioMatricardi\AppData\Local\Temp\pip-install-evovnb_5\llama-cpp-python_47ad063b60de4783b243782a39c9c6ac\vendor\llama.cpp\examples\llava\clip.cpp:  945][         clip_model_load] clip_model_load: - kv   9:                     clip.vision.patch_size u32              = 14
[1716096893] [C:\Users\FabioMatricardi\AppData\Local\Temp\pip-install-evovnb_5\llama-cpp-python_47ad063b60de4783b243782a39c9c6ac\vendor\llama.cpp\examples\llava\clip.cpp:  945][         clip_model_load] clip_model_load: - kv  10:               clip.vision.embedding_length u32              = 1152
[1716096893] [C:\Users\FabioMatricardi\AppData\Local\Temp\pip-install-evovnb_5\llama-cpp-python_47ad063b60de4783b243782a39c9c6ac\vendor\llama.cpp\examples\llava\clip.cpp:  945][         clip_model_load] clip_model_load: - kv  11:            clip.vision.feed_forward_length u32              = 4304
[1716096893] [C:\Users\FabioMatricardi\AppData\Local\Temp\pip-install-evovnb_5\llama-cpp-python_47ad063b60de4783b243782a39c9c6ac\vendor\llama.cpp\examples\llava\clip.cpp:  945][         clip_model_load] clip_model_load: - kv  12:                 clip.vision.projection_dim u32              = 2048
[1716096893] [C:\Users\FabioMatricardi\AppData\Local\Temp\pip-install-evovnb_5\llama-cpp-python_47ad063b60de4783b243782a39c9c6ac\vendor\llama.cpp\examples\llava\clip.cpp:  945][         clip_model_load] clip_model_load: - kv  13:           clip.vision.attention.head_count u32              = 16
[1716096893] [C:\Users\FabioMatricardi\AppData\Local\Temp\pip-install-evovnb_5\llama-cpp-python_47ad063b60de4783b243782a39c9c6ac\vendor\llama.cpp\examples\llava\clip.cpp:  945][         clip_model_load] clip_model_load: - kv  14:   clip.vision.attention.layer_norm_epsilon f32              = 0.000001
[1716096893] [C:\Users\FabioMatricardi\AppData\Local\Temp\pip-install-evovnb_5\llama-cpp-python_47ad063b60de4783b243782a39c9c6ac\vendor\llama.cpp\examples\llava\clip.cpp:  945][         clip_model_load] clip_model_load: - kv  15:                    clip.vision.block_count u32              = 28
[1716096893] [C:\Users\FabioMatricardi\AppData\Local\Temp\pip-install-evovnb_5\llama-cpp-python_47ad063b60de4783b243782a39c9c6ac\vendor\llama.cpp\examples\llava\clip.cpp:  945][         clip_model_load] clip_model_load: - kv  16:                     clip.vision.image_mean arr[f32,3]       = [0.500000, 0.500000, 0.500000]
[1716096893] [C:\Users\FabioMatricardi\AppData\Local\Temp\pip-install-evovnb_5\llama-cpp-python_47ad063b60de4783b243782a39c9c6ac\vendor\llama.cpp\examples\llava\clip.cpp:  945][         clip_model_load] clip_model_load: - kv  17:                      clip.vision.image_std arr[f32,3]       = [0.500000, 0.500000, 0.500000]
[1716096893] [C:\Users\FabioMatricardi\AppData\Local\Temp\pip-install-evovnb_5\llama-cpp-python_47ad063b60de4783b243782a39c9c6ac\vendor\llama.cpp\examples\llava\clip.cpp:  945][         clip_model_load] clip_model_load: - kv  18:                              clip.use_gelu bool             = true
[1716096893] [C:\Users\FabioMatricardi\AppData\Local\Temp\pip-install-evovnb_5\llama-cpp-python_47ad063b60de4783b243782a39c9c6ac\vendor\llama.cpp\examples\llava\clip.cpp:  954][         clip_model_load] clip_model_load: - type  f32:  285 tensors
[1716096893] [C:\Users\FabioMatricardi\AppData\Local\Temp\pip-install-evovnb_5\llama-cpp-python_47ad063b60de4783b243782a39c9c6ac\vendor\llama.cpp\examples\llava\clip.cpp:  954][         clip_model_load] clip_model_load: - type  f16:  172 tensors
[1716096893] [C:\Users\FabioMatricardi\AppData\Local\Temp\pip-install-evovnb_5\llama-cpp-python_47ad063b60de4783b243782a39c9c6ac\vendor\llama.cpp\examples\llava\clip.cpp: 1007][         clip_model_load] clip_model_load: CLIP using CPU backend
[1716096893] [C:\Users\FabioMatricardi\AppData\Local\Temp\pip-install-evovnb_5\llama-cpp-python_47ad063b60de4783b243782a39c9c6ac\vendor\llama.cpp\examples\llava\clip.cpp: 1039][         clip_model_load] clip_model_load: params backend buffer size =  821.83 MB (457 tensors)
[1716096894] [C:\Users\FabioMatricardi\AppData\Local\Temp\pip-install-evovnb_5\llama-cpp-python_47ad063b60de4783b243782a39c9c6ac\vendor\llama.cpp\examples\llava\clip.cpp:  149][             get_key_idx] key clip.vision.image_grid_pinpoints not found in file
[1716096894] [C:\Users\FabioMatricardi\AppData\Local\Temp\pip-install-evovnb_5\llama-cpp-python_47ad063b60de4783b243782a39c9c6ac\vendor\llama.cpp\examples\llava\clip.cpp:  149][             get_key_idx] key clip.vision.mm_patch_merge_type not found in file
[1716096894] [C:\Users\FabioMatricardi\AppData\Local\Temp\pip-install-evovnb_5\llama-cpp-python_47ad063b60de4783b243782a39c9c6ac\vendor\llama.cpp\examples\llava\clip.cpp:  149][             get_key_idx] key clip.vision.image_crop_resolution not found in file
[1716096894] [C:\Users\FabioMatricardi\AppData\Local\Temp\pip-install-evovnb_5\llama-cpp-python_47ad063b60de4783b243782a39c9c6ac\vendor\llama.cpp\examples\llava\clip.cpp: 1316][         clip_model_load] clip_model_load: compute allocated memory: 50.10 MB
[1716096928] encode_image_with_clip: image embedding created: 729 tokens
[1716096928] 
encode_image_with_clip: image encoded in 17554.84 ms by CLIP (   24.08 ms per image patch)
[1716097216] encode_image_with_clip: image embedding created: 729 tokens
[1716097216] 
encode_image_with_clip: image encoded in 17519.86 ms by CLIP (   24.03 ms per image patch)
[1716097483] encode_image_with_clip: image embedding created: 729 tokens
[1716097483] 
encode_image_with_clip: image encoded in 21888.16 ms by CLIP (   30.02 ms per image patch)
[1716097991] encode_image_with_clip: image embedding created: 729 tokens
[1716097991] 
encode_image_with_clip: image encoded in 21483.00 ms by CLIP (   29.47 ms per image patch)
[1716098261] encode_image_with_clip: image embedding created: 729 tokens
[1716098261] 
encode_image_with_clip: image encoded in 18735.70 ms by CLIP (   25.70 ms per image patch)
[1716098652] encode_image_with_clip: image embedding created: 729 tokens
[1716098652] 
encode_image_with_clip: image encoded in 18110.77 ms by CLIP (   24.84 ms per image patch)
